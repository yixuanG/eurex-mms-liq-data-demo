{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Colab environment setup and data preparation\n",
    "\n",
    "This notebook helps you:\n",
    "- Check Google Drive mount\n",
    "- Install dependencies and initialize DuckDB (via repository script)\n",
    "- Download interest rate data for IV calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e788d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Colab\n",
      "Drive root exists: True\n",
      "MyDrive exists: True\n",
      "Mounted: True\n"
     ]
    }
   ],
   "source": [
    "# 1) Check Google Drive mount (no automatic mount)\n",
    "import os\n",
    "\n",
    "# Detect if running inside Google Colab\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "DRIVE_ROOT = \"/content/drive\"\n",
    "MYDRIVE = os.path.join(DRIVE_ROOT, \"MyDrive\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    root_exists = os.path.exists(DRIVE_ROOT)\n",
    "    mydrive_exists = os.path.exists(MYDRIVE)\n",
    "    is_mounted = (os.path.ismount(DRIVE_ROOT) or mydrive_exists)\n",
    "\n",
    "    print(\"Environment: Colab\")\n",
    "    print(f\"Drive root exists: {root_exists}\")\n",
    "    print(f\"MyDrive exists: {mydrive_exists}\")\n",
    "    print(f\"Mounted: {is_mounted}\")\n",
    "\n",
    "    if not is_mounted:\n",
    "        print(\"\\n[Hint] Drive seems not mounted. If you need Drive, run:\")\n",
    "        print(\"from google.colab import drive\")\n",
    "        print(\"drive.mount('/content/drive')\")\n",
    "else:\n",
    "    print(\"Environment: not Colab. Skipping Google Drive mount check.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3f246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/00_EUREX/eurex-liquidity-demo'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Set repository path and validate\n",
    "import os\n",
    "REPO_DIR = '/content/drive/MyDrive/00_EUREX/eurex-liquidity-demo'\n",
    "assert os.path.exists(REPO_DIR), f'Repo not found: {REPO_DIR}'\n",
    "REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f47f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: /content/drive/MyDrive/00_EUREX/eurex-liquidity-demo/warehouse/setup_colab_duckdb.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 3) Install dependencies and initialize DuckDB (script)\n",
    "!bash \"$REPO_DIR/warehouse/setup_colab_duckdb.sh\" \"$REPO_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "458a02e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB version: 1.3.2\n",
      "   ok\n",
      "0   1\n"
     ]
    }
   ],
   "source": [
    "# 4) Verify DuckDB connection\n",
    "import duckdb, os\n",
    "db_path = os.path.join(REPO_DIR, 'warehouse', 'eurex.duckdb')\n",
    "con = duckdb.connect(db_path)\n",
    "print('DuckDB version:', duckdb.__version__)\n",
    "print(con.sql(\"SELECT 1 AS ok\").df())\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226eebc7",
   "metadata": {},
   "source": [
    "# 5) Extract original dataset\n",
    "\n",
    "Extract the original Eurex sample archive `Sample_Eurex_20201201_10MktSegID.tar.gz`\n",
    "from Google Drive into project folder `eurex-liquidity-demo/data_raw` or Colab local (`/content`)\n",
    "\n",
    "Instructions:\n",
    "- Set `TAR_PATH` to your `.tar.gz` on Drive.\n",
    "- Set `DEST_DIR` to your project folder `data_raw` or Colab local (`/content`).\n",
    "- Adjust `--progress-every`, `--list-top`, and `--show-tree` as needed.\n",
    "\n",
    "In the current demo, we only extract segments 48 and 50\n",
    "- 48: FSTK-ADSG (Futures on Adidas AG)\n",
    "- 50: OSTK-ADS (Options on Adidas AG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b007b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries: 81\n",
      "\n",
      "Sample_Eurex_20201201_10MktSegID\n",
      "Sample_Eurex_20201201_10MktSegID/821\n",
      "Sample_Eurex_20201201_10MktSegID/821/ISC_821_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/821/IS_821_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/821/MISC_821_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/821/DS_821_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/821/DI_821_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/821/PSC_821_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/589\n",
      "Sample_Eurex_20201201_10MktSegID/589/PSC_589_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/589/DS_589_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/589/MISC_589_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/589/DI_589_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/589/IS_589_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374\n",
      "Sample_Eurex_20201201_10MktSegID/1374/DS_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/DI_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/IS_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/MISC_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/PSC_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/CIU_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/ISC_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1374/II_1374_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373\n",
      "Sample_Eurex_20201201_10MktSegID/1373/DS_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/DI_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/II_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/MISC_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/ISC_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/CIU_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/QR_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/IS_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1373/PSC_1373_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/DataSample.xlsx\n",
      "Sample_Eurex_20201201_10MktSegID/48\n",
      "Sample_Eurex_20201201_10MktSegID/48/DI_48_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/48/PSC_48_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/48/DS_48_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/48/MISC_48_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/48/IS_48_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/48/ISC_48_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1209\n",
      "Sample_Eurex_20201201_10MktSegID/1209/DS_1209_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1209/IS_1209_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1209/PSC_1209_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1209/DI_1209_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1209/MISC_1209_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176\n",
      "Sample_Eurex_20201201_10MktSegID/1176/ISC_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/MISC_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/IS_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/DS_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/QR_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/CIU_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/II_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/DI_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/1176/PSC_1176_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/688\n",
      "Sample_Eurex_20201201_10MktSegID/688/DS_688_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/688/MISC_688_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/688/ISC_688_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/688/IS_688_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/688/DI_688_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/688/PSC_688_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/702\n",
      "Sample_Eurex_20201201_10MktSegID/702/IS_702_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/702/DI_702_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/702/PSC_702_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/702/DS_702_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/702/MISC_702_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50\n",
      "Sample_Eurex_20201201_10MktSegID/50/ISC_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/IS_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/DI_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/CR_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/PSC_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/CIU_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/II_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/DS_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/50/MISC_50_20201201.csv\n",
      "Sample_Eurex_20201201_10MktSegID/PS_20201201.csv\n"
     ]
    }
   ],
   "source": [
    "# # 5.1) Inspect tar.gz file structure (without extraction)\n",
    "# import tarfile\n",
    "\n",
    "# TAR_PATH = \"/content/drive/MyDrive/00_EUREX/sample_data/Sample_Eurex_20201201_10MktSegID.tar.gz\"\n",
    "\n",
    "# with tarfile.open(TAR_PATH, 'r:gz') as tar:\n",
    "#     names = tar.getnames()\n",
    "#     print(f\"Total entries: {len(names)}\\n\")\n",
    "#     for name in names:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Destination (Drive) usage:\n",
      "================================================================================\n",
      "Total: 225.83 GB  |  Used: 42.18 GB  |  Free: 183.64 GB\n",
      "\n",
      "Tar size: 2.57 GB\n",
      "\n",
      "[INFO] Extracting segments ['48', '50']\n",
      "[INFO] From: /content/drive/MyDrive/00_EUREX/sample_data/Sample_Eurex_20201201_10MktSegID.tar.gz\n",
      "[INFO] To  : content/data_raw\n",
      "Progress: 81/81 (100.0%)  |  Extracted: 15  |  Elapsed: 261.7s  \n",
      "================================================================================\n",
      "[OK] Extraction completed\n",
      "Elapsed: 261.7 s  |  Members scanned: 81  |  Extracted: 15\n",
      "\n",
      "Extracted files (under dest): 15  |  Total size: 1.28 GB\n",
      "\n",
      "================================================================================\n",
      "Top 20 files by size:\n",
      "================================================================================\n",
      "  1. Sample_Eurex_20201201_10MktSegID/50/DI_50_20201201.csv                    1.08 GB\n",
      "  2. Sample_Eurex_20201201_10MktSegID/50/DS_50_20201201.csv                  204.71 MB\n",
      "  3. Sample_Eurex_20201201_10MktSegID/48/DI_48_20201201.csv                    1.22 MB\n",
      "  4. Sample_Eurex_20201201_10MktSegID/48/DS_48_20201201.csv                  990.78 KB\n",
      "  5. Sample_Eurex_20201201_10MktSegID/50/IS_50_20201201.csv                  146.84 KB\n",
      "  6. Sample_Eurex_20201201_10MktSegID/48/IS_48_20201201.csv                    4.43 KB\n",
      "  7. Sample_Eurex_20201201_10MktSegID/50/II_50_20201201.csv                    1.00 KB\n",
      "  8. Sample_Eurex_20201201_10MktSegID/50/CIU_50_20201201.csv                  639.00 B\n",
      "  9. Sample_Eurex_20201201_10MktSegID/50/MISC_50_20201201.csv                 423.00 B\n",
      " 10. Sample_Eurex_20201201_10MktSegID/50/PSC_50_20201201.csv                  363.00 B\n",
      " 11. Sample_Eurex_20201201_10MktSegID/48/PSC_48_20201201.csv                  352.00 B\n",
      " 12. Sample_Eurex_20201201_10MktSegID/50/ISC_50_20201201.csv                  321.00 B\n",
      " 13. Sample_Eurex_20201201_10MktSegID/48/MISC_48_20201201.csv                 257.00 B\n",
      " 14. Sample_Eurex_20201201_10MktSegID/48/ISC_48_20201201.csv                  254.00 B\n",
      " 15. Sample_Eurex_20201201_10MktSegID/50/CR_50_20201201.csv                   199.00 B\n",
      "\n",
      "=== Directory Tree ===\n",
      "üìÅ content/data_raw/\n",
      "  üìÅ Sample_Eurex_20201201_10MktSegID/\n",
      "    üìÅ 48/\n",
      "      üìÑ DI_48_20201201.csv                                 (  1.22 MB)\n",
      "      üìÑ DS_48_20201201.csv                                 (990.78 KB)\n",
      "      üìÑ ISC_48_20201201.csv                                ( 254.00 B)\n",
      "      üìÑ IS_48_20201201.csv                                 (  4.43 KB)\n",
      "      üìÑ MISC_48_20201201.csv                               ( 257.00 B)\n",
      "      üìÑ PSC_48_20201201.csv                                ( 352.00 B)\n",
      "    üìÅ 50/\n",
      "      üìÑ CIU_50_20201201.csv                                ( 639.00 B)\n",
      "      üìÑ CR_50_20201201.csv                                 ( 199.00 B)\n",
      "      üìÑ DI_50_20201201.csv                                 (  1.08 GB)\n",
      "      üìÑ DS_50_20201201.csv                                 (204.71 MB)\n",
      "      üìÑ II_50_20201201.csv                                 (  1.00 KB)\n",
      "      üìÑ ISC_50_20201201.csv                                ( 321.00 B)\n",
      "      üìÑ IS_50_20201201.csv                                 (146.84 KB)\n",
      "      üìÑ MISC_50_20201201.csv                               ( 423.00 B)\n",
      "      üìÑ PSC_50_20201201.csv                                ( 363.00 B)\n"
     ]
    }
   ],
   "source": [
    "# '''\n",
    "# Uncomment the code below to extract the dataset to Colab local.\n",
    "# '''\n",
    "\n",
    "# # 6) Extract dataset to Colab local\n",
    "# # Set TAR_PATH and DEST_DIR, then run the command below.\n",
    "# # Example uses the file path you referenced earlier. Adjust if needed.\n",
    "# TAR_PATH = \"/content/drive/MyDrive/00_EUREX/sample_data/Sample_Eurex_20201201_10MktSegID.tar.gz\"\n",
    "# DEST_DIR = f\"content/data_raw\"\n",
    "\n",
    "# !python \"{REPO_DIR}/scripts/extract_segments_to_drive.py\" \\\n",
    "#   --tar \"{TAR_PATH}\" \\\n",
    "#   --dest \"{DEST_DIR}\" \\\n",
    "#   --segments 48 50 \\\n",
    "#   --progress-every 10 \\\n",
    "#   --list-top 20 \\\n",
    "#   --show-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259fda2",
   "metadata": {},
   "source": [
    "## Extract FULL dataset to Colab local SSD\n",
    "\n",
    "**Important storage strategy:**\n",
    "- **Raw data**: Extract to Colab local SSD (`/content/`) - temporary, fast, free space\n",
    "- **Intermediate products**: Save to Google Drive (slices, aggregates, Parquet files) - persistent\n",
    "\n",
    "This extraction includes ALL segments for full-day processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Disk usage on Colab local (/content/):\n",
      "================================================================================\n",
      "Total: 225.83 GB  |  Used: 38.28 GB  |  Free: 187.54 GB\n",
      "\n",
      "Tar size: 2.57 GB  |  Estimated need: 10.29 GB\n",
      "\n",
      "[INFO] Extracting: /content/drive/MyDrive/00_EUREX/sample_data/Sample_Eurex_20201201_10MktSegID.tar.gz\n",
      "[INFO] Destination: /content\n",
      "Members: 81\n",
      "================================================================================\n",
      "Progress: 50/81 (61.7%)  |  Elapsed: 154.9s  |  Sample_Eurex_20201201_10MktSegID/1176/MISC_1176_20201201.csv"
     ]
    }
   ],
   "source": [
    "# Extract FULL dataset to Colab local SSD (all segments)\n",
    "TAR_PATH = \"/content/drive/MyDrive/00_EUREX/sample_data/Sample_Eurex_20201201_10MktSegID.tar.gz\"\n",
    "DEST_LOCAL = \"/content\"  # Colab local SSD - fast and free\n",
    "\n",
    "!python \"{REPO_DIR}/scripts/extract_to_colab_local.py\" \\\n",
    "  --tar \"{TAR_PATH}\" \\\n",
    "  --dest \"{DEST_LOCAL}\"\n",
    "\n",
    "# Expected output: /content/Sample_Eurex_20201201/\n",
    "RAW_LOCAL = \"/content/Sample_Eurex_20201201\"\n",
    "print(f\"\\n‚úÖ Raw data extracted to: {RAW_LOCAL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda1b70",
   "metadata": {},
   "source": [
    "## Create symlink to raw data folder\n",
    "\n",
    "Create a shortcut in your Drive repo pointing to the Colab local raw data.\n",
    "This makes it easy to navigate the raw data structure from the Drive repo folder.\n",
    "\n",
    "**üìñ See `data_raw/COLAB_RAW_DATA_STRUCTURE.md` for full structure documentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641de93",
   "metadata": {},
   "source": [
    "### Alternative: Detailed depth analysis (slower but more accurate)\n",
    "\n",
    "Use the `check_max_depth.py` script for precise depth detection.\n",
    "This requires first inferring the schema for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7585600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 segments: [48, 50, 589, 688, 702, 821, 1176, 1209, 1373, 1374]\n",
      "======================================================================\n",
      "   48: Max level:  5 (L1: 35.0%)\n",
      "   50: Max level:  5 (L1: 25.2%)\n",
      "  589: Max level:  5 (L1: 42.6%)\n",
      "  688: Max level:  2 (L1: 37.4%)\n",
      "  702: Max level:  5 (L1: 73.3%)\n",
      "  821: Max level:  5 (L1: 63.0%)\n",
      " 1176: Max level:  5 (L1: 19.2%)\n",
      " 1209: Max level:  4 (L1: 34.3%)\n",
      " 1373: Max level:  5 (L1: 10.7%)\n",
      " 1374: Max level:  5 (L1: 11.8%)\n",
      "======================================================================\n",
      "\n",
      "üìä DEPTH SUMMARY TABLE\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9f7b04bb-c82c-4d79-b565-a35056286646\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>Max_Level</th>\n",
       "      <th>Suggested_L</th>\n",
       "      <th>L1_Entries_%</th>\n",
       "      <th>DI_Size_MB</th>\n",
       "      <th>Lines_Scanned</th>\n",
       "      <th>Total_Entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5000</td>\n",
       "      <td>13415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>25.2</td>\n",
       "      <td>1103.96</td>\n",
       "      <td>5000</td>\n",
       "      <td>153570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>42.6</td>\n",
       "      <td>221.08</td>\n",
       "      <td>5000</td>\n",
       "      <td>8809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>688</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>37.4</td>\n",
       "      <td>366.51</td>\n",
       "      <td>5000</td>\n",
       "      <td>9453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>702</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>73.3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>524</td>\n",
       "      <td>7281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>821</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>63.0</td>\n",
       "      <td>54.77</td>\n",
       "      <td>5000</td>\n",
       "      <td>19668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1176</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9610.09</td>\n",
       "      <td>5000</td>\n",
       "      <td>149313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1209</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>34.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1373</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10.7</td>\n",
       "      <td>921.57</td>\n",
       "      <td>5000</td>\n",
       "      <td>39015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1374</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.38</td>\n",
       "      <td>682</td>\n",
       "      <td>6873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f7b04bb-c82c-4d79-b565-a35056286646')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9f7b04bb-c82c-4d79-b565-a35056286646 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9f7b04bb-c82c-4d79-b565-a35056286646');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   Segment  Max_Level  Suggested_L  L1_Entries_%  DI_Size_MB  Lines_Scanned  \\\n",
       "0       48          5            6          35.0        1.22           5000   \n",
       "1       50          5            6          25.2     1103.96           5000   \n",
       "2      589          5            6          42.6      221.08           5000   \n",
       "3      688          2            3          37.4      366.51           5000   \n",
       "4      702          5            6          73.3        0.46            524   \n",
       "5      821          5            6          63.0       54.77           5000   \n",
       "6     1176          5            6          19.2     9610.09           5000   \n",
       "7     1209          4            5          34.3        0.00              4   \n",
       "8     1373          5            6          10.7      921.57           5000   \n",
       "9     1374          5            6          11.8        0.38            682   \n",
       "\n",
       "   Total_Entries  \n",
       "0          13415  \n",
       "1         153570  \n",
       "2           8809  \n",
       "3           9453  \n",
       "4           7281  \n",
       "5          19668  \n",
       "6         149313  \n",
       "7             35  \n",
       "8          39015  \n",
       "9           6873  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà DEPTH DISTRIBUTION:\n",
      "----------------------------------------------------------------------\n",
      "  Segment    48: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment    50: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment   589: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment   688: L 2  üü¢ Low depth (suitable for L5)\n",
      "  Segment   702: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment   821: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment  1176: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment  1209: L 4  üü¢ Low depth (suitable for L5)\n",
      "  Segment  1373: L 5  üü¢ Low depth (suitable for L5)\n",
      "  Segment  1374: L 5  üü¢ Low depth (suitable for L5)\n",
      "\n",
      "Summary saved to: /content/drive/MyDrive/00_EUREX/eurex-liquidity-demo/data_raw/segment_depth_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Inspect maximum depth for all segments (PROPER PARSING)\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path(REPO_DIR) / \"src\"))\n",
    "\n",
    "from eurex_liquidity.parser import extract_entry_tokens_from_di_line, infer_di_mapping, tokens_to_event\n",
    "\n",
    "RAW_LOCAL = \"/content/Sample_Eurex_20201201_10MktSegID\"\n",
    "\n",
    "# Find all segments\n",
    "segment_dirs = [d for d in Path(RAW_LOCAL).iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "segments = sorted([int(d.name) for d in segment_dirs])\n",
    "\n",
    "print(f\"Found {len(segments)} segments: {segments}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for seg in segments:\n",
    "    seg_dir = Path(RAW_LOCAL) / str(seg)\n",
    "    \n",
    "    # Find DI file\n",
    "    di_files = list(seg_dir.glob(\"DI_*.csv\"))\n",
    "    if not di_files:\n",
    "        print(f\"Segment {seg:5d}: ‚ö†Ô∏è  No DI file found\")\n",
    "        continue\n",
    "    \n",
    "    di_file = di_files[0]\n",
    "    file_size_mb = di_file.stat().st_size / 1024 / 1024\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Infer schema from DI file\n",
    "        print(f\"{seg:5d}:\", end=\"\", flush=True)\n",
    "        \n",
    "        # Read first 200 lines for schema inference\n",
    "        sample_lines = []\n",
    "        with open(di_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 200:\n",
    "                    break\n",
    "                sample_lines.append(line.strip())\n",
    "        \n",
    "        mapping = infer_di_mapping(sample_lines, sample_limit=200)\n",
    "        \n",
    "        if mapping is None:\n",
    "            print(f\" ‚ùå Schema inference failed\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Parse entries and collect price levels\n",
    "        level_counts = Counter()\n",
    "        lines_scanned = 0\n",
    "        sample_limit = 5000\n",
    "        \n",
    "        with open(di_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= sample_limit:\n",
    "                    break\n",
    "                lines_scanned += 1\n",
    "                \n",
    "                entries = extract_entry_tokens_from_di_line(line.strip())\n",
    "                for entry_tokens in entries:\n",
    "                    evt = tokens_to_event(entry_tokens, mapping)\n",
    "                    level = evt.get('price_level')\n",
    "                    \n",
    "                    if level is not None and isinstance(level, int) and 0 <= level <= 100:\n",
    "                        level_counts[level] += 1\n",
    "        \n",
    "        max_level = max(level_counts.keys()) if level_counts else 0\n",
    "        \n",
    "        # Calculate distribution\n",
    "        l1_pct = (level_counts.get(0, 0) / sum(level_counts.values()) * 100) if level_counts else 0\n",
    "        \n",
    "        results.append({\n",
    "            'Segment': seg,\n",
    "            'Max_Level': max_level,\n",
    "            'Suggested_L': min(max_level + 1, 20),\n",
    "            'L1_Entries_%': round(l1_pct, 1),\n",
    "            'DI_Size_MB': round(file_size_mb, 2),\n",
    "            'Lines_Scanned': lines_scanned,\n",
    "            'Total_Entries': sum(level_counts.values())\n",
    "        })\n",
    "        \n",
    "        print(f\" Max level: {max_level:2d} (L1: {l1_pct:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create summary DataFrame\n",
    "if results:\n",
    "    df_depth = pd.DataFrame(results).sort_values('Segment')\n",
    "    print(\"\\nüìä DEPTH SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    display(df_depth)\n",
    "    \n",
    "    # Distribution stats\n",
    "    print(\"\\nüìà DEPTH DISTRIBUTION:\")\n",
    "    print(\"-\"*70)\n",
    "    for _, row in df_depth.iterrows():\n",
    "        seg = int(row['Segment'])\n",
    "        max_lvl = int(row['Max_Level'])\n",
    "        if max_lvl <= 5:\n",
    "            cat = \"üü¢ Low depth (suitable for L5)\"\n",
    "        elif max_lvl <= 10:\n",
    "            cat = \"üü° Medium depth (use L10)\"\n",
    "        elif max_lvl <= 15:\n",
    "            cat = \"üü† High depth (use L15)\"\n",
    "        else:\n",
    "            cat = \"üî¥ Very deep (use L20)\"\n",
    "        print(f\"  Segment {seg:5d}: L{max_lvl:2d}  {cat}\")\n",
    "    \n",
    "    # Save summary to Drive\n",
    "    summary_path = f\"{REPO_DIR}/data_raw/segment_depth_summary.json\"\n",
    "    summary_data = {\n",
    "        'analysis_date': pd.Timestamp.now().isoformat(),\n",
    "        'segments_analyzed': len(results),\n",
    "        'results': results  # Use raw results list instead of DataFrame\n",
    "    }\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary_data, f, indent=2)\n",
    "    print(f\"\\nSummary saved to: {summary_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No segments processed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
